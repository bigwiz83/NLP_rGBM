{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
      "Requirement already satisfied: transformers in e:\\anaconda\\lib\\site-packages (4.2.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda\\lib\\site-packages (from datasets) (0.8.3)\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\lib\\site-packages (from datasets) (1.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\anaconda\\lib\\site-packages (from datasets) (2.21.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp38-cp38-win_amd64.whl (35 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py38-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda\\lib\\site-packages (from datasets) (1.19.2)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.7-py3-none-any.whl (33 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-3.0.0-cp38-cp38-win_amd64.whl (12.7 MB)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in e:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in e:\\anaconda\\lib\\site-packages (from seqeval) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in e:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in e:\\anaconda\\lib\\site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: sacremoses in e:\\anaconda\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\anaconda\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in e:\\anaconda\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16175 sha256=93cb7eda31e28308c5e9cb4dc9dd3fe5a8bb4fdab9db06fe8e0ea8cdd4d6b096\n",
      "  Stored in directory: c:\\users\\관리자\\appdata\\local\\pip\\cache\\wheels\\ad\\5c\\ba\\05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built seqeval\n",
      "Installing collected packages: tqdm, dill, xxhash, pyarrow, multiprocess, huggingface-hub, seqeval, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.57.0\n",
      "    Uninstalling tqdm-4.57.0:\n",
      "      Successfully uninstalled tqdm-4.57.0\n",
      "Successfully installed datasets-1.5.0 dill-0.3.3 huggingface-hub-0.0.7 multiprocess-0.70.11.1 pyarrow-3.0.0 seqeval-1.2.2 tqdm-4.49.0 xxhash-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyserini 0.11.0.0 requires tqdm>=4.56.0, but you have tqdm 4.49.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/master/notebooks.html\n",
    "!pip install datasets transformers seqeval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "traindata = []\n",
    "testdata = []\n",
    "\n",
    "# root = 'E:/2021 NLP/BIOCorpus/Chemical'\n",
    "root = 'E:/2021 NLP/BIOCorpus/Gene'\n",
    "\n",
    "for (path1, dir, files) in os.walk(root):\n",
    "    for i in files:\n",
    "        name, ext = os.path.splitext(i)\n",
    "        if name=='train' or name=='valid':\n",
    "            round = Path(path1+'/'+name+ext)\n",
    "            raw_text = round.read_text().strip()\n",
    "            raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "            traindata.append(raw_docs)\n",
    "        elif name=='test':\n",
    "            round = Path(path1+'/'+name+ext)\n",
    "            raw_text = round.read_text().strip()\n",
    "            raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "            testdata.append(raw_docs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Making merged Train/Validation vs. Test set\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, pdb\n",
    "\n",
    "def read_wnut(merged):\n",
    "    # file_path = Path(file_path)\n",
    "    # raw_text = file_path.read_text().strip()\n",
    "    # raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    # https://github.com/BaderLab/Biomedical-Corpora\n",
    "    for raw_docs in merged:\n",
    "        for doc in raw_docs:\n",
    "            tokens = []\n",
    "            tags = []\n",
    "            for line in doc.split('\\n'):\n",
    "                token, tag = line.split('\\t')\n",
    "#                if tag not in ['B-CHED','B-CLLN','B-DISO', 'B-LIVB', 'B-PRGE', 'B-CLTP', 'O', 'I-CHED','I-CLLN','I-DISO','I-LIVB','I-PRGE','I-CLTP']:\n",
    "#                if tag not in ['B-CHED', 'O','I-CHED']:\n",
    "                if tag not in ['B-PRGE', 'O','I-PRGE']:\n",
    "                    tag ='O'\n",
    "                tokens.append(token)\n",
    "                tags.append(tag)\n",
    "            token_docs.append(tokens)\n",
    "            tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "train_texts, train_tags = read_wnut(traindata)\n",
    "val_texts, val_tags = read_wnut(testdata)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)\n",
    "\n",
    "unique_tags = set(tag for doc in train_tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(sorted(unique_tags,reverse=True))}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        truelabel_length = len ( doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] )\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels[:truelabel_length]\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = BioDataset(train_encodings, train_labels)\n",
    "val_dataset = BioDataset(val_encodings, val_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import concatenate_datasets, ReadInstruction, load_dataset, load_metric, list_datasets, list_metrics\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", num_labels=len(unique_tags))\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "#    pdb.set_trace()\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('NER.p', 'wb') as file:    # james.p 파일을 바이너리 쓰기 모드(wb)로 열기\n",
    "#     pickle.dump(train_dataset, file)\n",
    "#     pickle.dump(val_dataset, file)\n",
    "#     pickle.dump(compute_metrics, file)\n",
    "\n",
    "# with open('NER.p', 'rb') as file:    # james.p 파일을 바이너리 읽기 모드(rb)로 열기\n",
    "#     train_dataset = pickle.load(file)\n",
    "#     val_dataset = pickle.load(file)\n",
    "#     compute_metrics = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{task}\",\n",
    "#    output_dir=\"Trained2/\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "#    label_names=id2tag,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.1,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10)\n",
    "\n",
    "# https://huggingface.co/transformers/master/main_classes/trainer.html\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model('E:/2021 NLP/Trained/')\n",
    "trainer.tokenizer.save_pretrained('E:/2021 NLP/Trained/')\n",
    "trainer.tokenizer.save_vocabulary('E:/2021 NLP/Trained/')\n",
    "\n",
    "with open('id2tag.pickle','wb') as fw:\n",
    "    pickle.dump(id2tag, fw)\n",
    "\n",
    "with open('tag2id.pickle','wb') as fw:\n",
    "    pickle.dump(tag2id , fw)\n",
    "\n",
    "with open('uniquetags.pickle','wb') as fw:\n",
    "    pickle.dump(unique_tags , fw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Evaluation\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"E:/2021 NLP/Trained_Gene/\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"E:/2021 NLP/Trained_Gene\")\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "example = \"Glioblastoma and lung cancer can be treated with anti-VEGF inhibitors.\"\n",
    "example=\"Postoperative radiotherapy combined with chemotherapy is a commonly used treatment for glioblastoma (GBM) but radiotherapy often fails to achieve the expected results mainly due to tumor radioresistance. In this study, we established a radioresistant subline from human glioma cell line U251 and found that Cathepsin D (CTSD), a gene closely related to the clinical malignancy and prognosis in glioma, had higher expression level in radioresistant clones than that in parental cells, and knocking down CTSD by small interfering RNA (siRNA) or its inhibitor Pepstatin‐A increased the radiosensitivity. The level of autophagy was enhanced in the radioresistant GBM cells compared with its parent cells, and silencing autophagy by light chain 3 (LC3) siRNA significantly sensitized GBM cells to ionizing radiation (IR). Moreover, the protein expression level of CTSD was positively correlated with the autophagy marker LC3 II/I and negatively correlated with P62 after IR in radioresistant cells. As expected, through the combination of Western blot and immunofluorescence assays, inhibition of CTSD increased the formation of autophagosomes, while decreased the formation of autolysosomes, which indicating an attenuated autophagy level, leading to radiosensitization ultimately. Our results revealed for the first time that CTSD regulated the radiosensitivity of glioblastoma by affecting the fusion of autophagosomes and lysosomes. In significance, CTSD might be a potential molecular biomarker and a new therapeutic target in glioblastoma.\"\n",
    "\n",
    "example=\"Postoperative radiotherapy combined with chemotherapy is a commonly used treatment for glioblastoma (GBM) but radiotherapy often fails to achieve the expected results mainly due to tumor radioresistance.\"\n",
    "        \"In this study, we established a radioresistant subline from human glioma cell line U251 and found that Cathepsin D (CTSD), a gene closely related to the clinical malignancy and prognosis in glioma, had higher expression level in radioresistant clones than that in parental cells, and knocking down CTSD by small interfering RNA (siRNA) or its inhibitor Pepstatin‐A increased the radiosensitivity. \" \\\n",
    "        \"The level of autophagy was enhanced in the radioresistant GBM cells compared with its parent cells, and silencing autophagy by light chain 3 (LC3) siRNA significantly sensitized GBM cells to ionizing radiation (IR). \" \\\n",
    "        \"Moreover, the protein expression level of CTSD was positively correlated with the autophagy marker LC3 II/I and negatively correlated with P62 after IR in radioresistant cells. \" \\\n",
    "        \"As expected, through the combination of Western blot and immunofluorescence assays, inhibition of CTSD increased the formation of autophagosomes, while decreased the formation of autolysosomes, which indicating an attenuated autophagy level, leading to radiosensitization ultimately. \" \\\n",
    "\n",
    "example = \"NF1 deficiency was identified as a probable cause of differential chemotaxis in mesenchymal GBMs. Other alterations, such as phosphatase and tensin homolog (PTEN) deletion in glioma cells, have been shown to activate the transcription factor Yes-associated protein 1, which directly upregulates LOX expression. LOX is a po- tent chemokine recruiting macrophages via activation of the β1 integrin/proline-rich tyrosine kinase 2 pathway in macrophages. Inhibition of LOX suppresses macrophage infiltration and tumor progression specifically in PTEN-null glioma models.60 Amplification of the epidermal growth factor receptor (EGFR) gene and its truncation mutant EGFR variant (v)III is another common genetic alteration in glioblastoma. EGFR and EGFRvIII cooperate to recruit macrophages in GBM via induction of chemokine MCP-1.\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f98f4e99",
   "language": "python",
   "display_name": "PyCharm (2021 NLP)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}